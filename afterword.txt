4. Best approach for YOUR pipeline (concrete & safe)
✅ Option A — Dual-stage retrieval (HIGHLY recommended)

Stage 1: Summary-based prefilter

Embed summaries only (very small vectors)

Do a fast similarity check with the query

Select top N chunks by summary meaning

Stage 2: Full-chunk reranking

Fetch full content for those N chunks

Pass full chunk text to reranker LLM

Generate answer from full chunks only

Why this works:

Summaries improve recall

Full text preserves precision

This would have caught Chunk 13 instantly for “claims procedure”.

✅ Option B — Hybrid embedding (safe improvement)

Instead of embedding just chunk text, embed:

[SUMMARY]
<summary text>

[CONTENT]
<first 500–700 chars of chunk>


Benefits:

Summary boosts intent matching

Content preserves detail

No risk of losing critical clauses

This is a very common production trick.